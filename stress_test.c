#include <assert.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>

#include "ememory.h"
#include "processor.h"  

// All tests below were generated by ChatGPT.

// ememory functions from ememory.c
// There were copied over rather than being included in the header file
// because they should only be used internally.
void read_header(char* data, uint16_t* size, uint16_t* next, uint16_t offset) {
	memcpy(size, data + offset, sizeof(uint16_t));
	memcpy(next, data + offset + sizeof(uint16_t), sizeof(uint16_t));
}

/* ========= Tunables ========= */
#ifndef TEST_MEM_SIZE
#define TEST_MEM_SIZE 1024     /* total bytes of backing buffer */
#endif

#define MAX_ALLOCS       512   /* max live allocations tracked */
#define N_ITERATIONS     50000 /* number of random operations */
#define P_ALLOC_PERCENT  65    /* % chance to do an allocation (else free) */
#define MAX_REQ_SIZE     64    /* random alloc size upper bound (inclusive) */
#define DO_PATTERN_CHECK 0     /* set to 0 to disable memory pattern checks */

/* ========= Small RNG (xorshift64*) ========= */
typedef struct { uint64_t s; } rng_t;
static inline void rng_seed(rng_t* r, uint64_t seed) { r->s = seed ? seed : 0x9E3779B97F4A7C15ULL; }
static inline uint64_t rng_next(rng_t* r) {
    uint64_t x = r->s;
    x ^= x >> 12; x ^= x << 25; x ^= x >> 27;
    r->s = x;
    return x * 0x2545F4914F6CDD1DULL;
}
static inline uint32_t rng_u32(rng_t* r, uint32_t n) { return (uint32_t)(rng_next(r) % (n ? n : 1)); }
static inline uint32_t rng_range(rng_t* r, uint32_t lo, uint32_t hi) { /* inclusive */
    return lo + rng_u32(r, (hi - lo + 1));
}

/* ========= Helpers ========= */
static int is_null_eptr(struct eptr p) { return p.ptr == 0 && p.size == 0; }

/* Track live allocations */
typedef struct {
    struct eptr p;
    int in_use;
    uint8_t tag;  /* byte pattern we wrote into this allocation */
} alloc_t;

typedef struct {
    char data[TEST_MEM_SIZE];
    struct ememory mem;
} heap_t;

static void heap_init(heap_t* h) {
    memset(h->data, 0, sizeof(h->data));
    memset(&h->mem, 0, sizeof(h->mem));
    h->mem.data = h->data;
    init_ememory(&h->mem, TEST_MEM_SIZE);
}

/* Walk free list; verify ordering & merging; return total free size */
static uint32_t check_free_list_and_sum(struct ememory* m) {
    char* data = m->data;
    uint16_t curr = m->free_head;
    uint16_t last_end = 0;
    uint32_t total = 0;

    /* Head must be either 0 or within heap */
    if (curr != 0) {
        assert(curr >= STARTING_OFFSET && curr < TEST_MEM_SIZE);
    }

    while (curr) {
        uint16_t sz, nxt;
        read_header(data, &sz, &nxt, curr);

        /* basic sanity */
        assert(sz > 0);
        assert(curr >= STARTING_OFFSET && curr + sz <= TEST_MEM_SIZE);

        /* strictly increasing addresses */
        if (last_end != 0) {
            assert(curr > last_end); /* NOT >= â€” adjacent blocks must be merged */
        }

        /* merging invariant: no adjacent free blocks allowed */
        if (nxt != 0) {
            assert(nxt > curr); /* forward */
            /* If nxt equals exact end of current, they should have been merged */
            assert(curr + sz < nxt);
        }

        total += sz;
        last_end = curr + sz;
        curr = nxt;
    }
    return total;
}

/* Verify live allocations don't overlap and are in bounds */
static uint32_t check_allocs(alloc_t* A, int n) {
    /* sum sizes & pairwise overlap check */
    uint32_t sum = 0;
    for (int i = 0; i < n; ++i) {
        if (!A[i].in_use) continue;
        struct eptr p = A[i].p;
        assert(p.size > 0);
        assert(p.ptr >= STARTING_OFFSET);
        assert(p.ptr + p.size <= TEST_MEM_SIZE);
        sum += p.size;

        for (int j = i + 1; j < n; ++j) {
            if (!A[j].in_use) continue;
            struct eptr q = A[j].p;
            uint32_t a0 = p.ptr, a1 = p.ptr + p.size;
            uint32_t b0 = q.ptr, b1 = q.ptr + q.size;
            /* overlap if max(start) < min(end) */
            assert(!( (a0 < b1) && (b0 < a1) ));
        }
    }
    return sum;
}

/* Spot-check pattern integrity to catch accidental overwrite */
static void check_patterns(heap_t* H, alloc_t* A, int n) {
#if DO_PATTERN_CHECK
    for (int i = 0; i < n; ++i) {
        if (!A[i].in_use) continue;
        struct eptr p = A[i].p;
        uint8_t tag = A[i].tag;
        char* base = H->data + p.ptr;

        /* sample up to 4 positions: start, quarter, mid, end-1 */
        size_t len = p.size;
        size_t idxs[4] = {0, len/4, len/2, (len? len-1: 0)};
        for (int k = 0; k < 4; ++k) {
            size_t idx = idxs[k];
            if (idx >= len) continue;
            assert((uint8_t)base[idx] == tag);
        }
    }
#else
    (void)H; (void)A; (void)n;
#endif
}

/* Write a pattern to an allocated region */
static void fill_pattern(heap_t* H, struct eptr p, uint8_t tag) {
#if DO_PATTERN_CHECK
    memset(H->data + p.ptr, (int)tag, p.size);
#else
    (void)H; (void)p; (void)tag;
#endif
}

/* Pick a random live slot index; return -1 if none */
static int pick_live_index(rng_t* R, alloc_t* A, int n) {
    int live = 0;
    for (int i = 0; i < n; ++i) if (A[i].in_use) ++live;
    if (!live) return -1;
    int k = rng_u32(R, live);
    for (int i = 0; i < n; ++i) {
        if (!A[i].in_use) continue;
        if (k-- == 0) return i;
    }
    return -1; /* should not happen */
}

/* Find a free slot for a new allocation record; return -1 if full */
static int pick_free_slot(alloc_t* A, int n) {
    for (int i = 0; i < n; ++i) if (!A[i].in_use) return i;
    return -1;
}

int main(void) {
    /* Seed control */
    uint64_t seed = 0;
    const char* senv = getenv("SEED");
    if (senv && *senv) {
        seed = strtoull(senv, NULL, 10);
    } else {
        seed = (uint64_t)time(NULL);
    }
    rng_t R; rng_seed(&R, seed);

    printf("Stress test: TEST_MEM_SIZE=%d, iterations=%d, seed=%llu\n",
           TEST_MEM_SIZE, N_ITERATIONS, (unsigned long long)seed);

    heap_t H; heap_init(&H);
    alloc_t A[MAX_ALLOCS] = {0};

    /* Initial invariants */
    {
        uint32_t free_sum = check_free_list_and_sum(&H.mem);
        assert(free_sum == (uint32_t)(TEST_MEM_SIZE - STARTING_OFFSET));
        assert(check_allocs(A, MAX_ALLOCS) == 0);
    }

    int ops = N_ITERATIONS;
    for (int step = 1; step <= ops; ++step) {
        int do_alloc = (rng_u32(&R, 100) < P_ALLOC_PERCENT);

        if (do_alloc) {
            int slot = pick_free_slot(A, MAX_ALLOCS);
            if (slot < 0) do_alloc = 0; /* no tracking space; force free */
        }

        if (do_alloc) {
            /* Occasionally try a big allocation to force exact-fit/exhaustion */
            uint32_t req;
            if (rng_u32(&R, 1000) == 0) {
                /* Rare: try near whole heap */
                req = rng_range(&R, (TEST_MEM_SIZE - STARTING_OFFSET) / 2, TEST_MEM_SIZE - STARTING_OFFSET);
            } else {
                req = rng_range(&R, 1, MAX_REQ_SIZE);
            }

            struct eptr p = emalloc(&H.mem, (uint16_t)req);
            if (!is_null_eptr(p)) {
                int slot = pick_free_slot(A, MAX_ALLOCS);
                assert(slot >= 0); /* we ensured above */
                A[slot].p = p;
                A[slot].in_use = 1;
                A[slot].tag = (uint8_t)(rng_u32(&R, 255) + 1); /* avoid 0 so we can see it */
                fill_pattern(&H, p, A[slot].tag);
            }
            /* If allocation failed, that's okay; continue */
        } else {
            int idx = pick_live_index(&R, A, MAX_ALLOCS);
            if (idx >= 0) {
                efree(&H.mem, A[idx].p);
                A[idx].in_use = 0;
            }
            /* If nothing live, skip */
        }

        /* Invariants after each op */
        uint32_t free_sum = check_free_list_and_sum(&H.mem);
        uint32_t alloc_sum = check_allocs(A, MAX_ALLOCS);
        check_patterns(&H, A, MAX_ALLOCS);

        /* Accounting: free + live == total capacity */
        uint32_t capacity = (uint32_t)(TEST_MEM_SIZE - STARTING_OFFSET);
        if (free_sum + alloc_sum != capacity) {
            /* Helpful dump before asserting */
            fprintf(stderr, "\n[ERROR] Accounting mismatch at step %d (seed=%llu)\n",
                    step, (unsigned long long)seed);
            fprintf(stderr, "free_sum=%u alloc_sum=%u capacity=%u\n",
                    free_sum, alloc_sum, capacity);
            /* Walk free list for debugging */
            uint16_t curr = H.mem.free_head;
            while (curr) {
                uint16_t sz, nxt;
                read_header(H.data, &sz, &nxt, curr);
                fprintf(stderr, "  free {ptr=%u size=%u next=%u}\n", curr, sz, nxt);
                curr = nxt;
            }
            /* Dump a few live allocs */
            int printed = 0;
            for (int i = 0; i < MAX_ALLOCS && printed < 16; ++i) {
                if (A[i].in_use) {
                    fprintf(stderr, "  live {ptr=%u size=%u tag=%u}\n",
                            A[i].p.ptr, A[i].p.size, A[i].tag);
                    ++printed;
                }
            }
            assert(0 && "Accounting invariant failed");
        }
    }

    /* Final full free, then re-verify coalescing back to a single free block */
    for (int i = 0; i < MAX_ALLOCS; ++i) {
        if (A[i].in_use) {
            efree(&H.mem, A[i].p);
            A[i].in_use = 0;
        }
    }
    uint32_t free_sum = check_free_list_and_sum(&H.mem);
    assert(free_sum == (uint32_t)(TEST_MEM_SIZE - STARTING_OFFSET));

    printf("OK: %d randomized operations; invariants held; seed=%llu\n",
           N_ITERATIONS, (unsigned long long)seed);
    return 0;
}
